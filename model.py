# Gerekli kÃ¼tÃ¼phaneleri yÃ¼klÃ¼yoruz
import pandas as pd  # Veri iÅŸleme iÃ§in
import numpy as np   # SayÄ±sal hesaplamalar iÃ§in
# Makine Ã¶ÄŸrenmesi algoritmalarÄ± iÃ§in sklearn kÃ¼tÃ¼phaneleri
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold  # Veriyi bÃ¶lme ve doÄŸrulama iÃ§in
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error  # Performans Ã¶lÃ§me iÃ§in
from sklearn.preprocessing import RobustScaler, PowerTransformer, LabelEncoder, PolynomialFeatures  # Veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in
from sklearn.pipeline import Pipeline  # Ä°ÅŸlem zincirleri iÃ§in
from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression, RFE  # Ã–zellik seÃ§imi iÃ§in
from sklearn.linear_model import Ridge  # DoÄŸrusal regresyon iÃ§in
from scipy import stats  # Ä°statistiksel iÅŸlemler iÃ§in
from scipy.stats import boxcox  # Veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in
import joblib  # Model kaydetme/yÃ¼kleme iÃ§in
import os  # Dosya iÅŸlemleri iÃ§in
import matplotlib.pyplot as plt  # Grafik Ã§izme iÃ§in
import seaborn as sns  # GÃ¼zel grafikler iÃ§in
import warnings
warnings.filterwarnings('ignore')  # UyarÄ±larÄ± gizle


def load_and_preprocess_data():
    """Veri setini yÃ¼kle ve Ã¶n iÅŸle - Ä°yileÅŸtirilmiÅŸ veri temizleme"""
    try:
        # Excel dosyasÄ±nÄ± pandas ile oku
        df = pd.read_excel('istanbul_konut2.xlsx')
        
        # SÃ¼tun isimlerini TÃ¼rkÃ§e karakterler ve boÅŸluklar olmadan dÃ¼zenle (consistency iÃ§in)
        df.columns = ['fiyat', 'ilce', 'mahalle', 'metrekare', 'oda_sayisi', 'yas', 'bulundugu_kat']
        
        # KaÃ§ tane kayÄ±t yÃ¼klendiÄŸini ekrana yazdÄ±r
        print(f"Ä°lk yÃ¼kleme: {df.shape[0]} kayÄ±t")
        
        # Eksik (NaN) verileri temizle - pandas dropna() ile
        df = df.dropna()  # Eksik verileri kaldÄ±r
        
        # Fiyat sÃ¼tununu sayÄ±sal veriye dÃ¶nÃ¼ÅŸtÃ¼r, hata varsa NaN yap
        df['fiyat'] = pd.to_numeric(df['fiyat'], errors='coerce')
        # FiyatÄ± eksik/hatalÄ± olan kayÄ±tlarÄ± sil
        df = df.dropna(subset=['fiyat'])  # FiyatÄ± eksik olan kayÄ±tlarÄ± Ã§Ä±kar
        
        # Ä°lÃ§e ve mahalle deÄŸerlerini standartlaÅŸtÄ±r ve tutarlÄ± hale getir
        df['ilce'] = df['ilce'].str.strip().str.title()      # BaÅŸÄ±na-sonuna boÅŸluk sil, ilk harfi bÃ¼yÃ¼t
        df['mahalle'] = df['mahalle'].str.strip().str.title()  # AynÄ± iÅŸlemi mahalle iÃ§in de yap
        
        # AynÄ± anlama gelebilecek farklÄ± yazÄ±mlarÄ± birleÅŸtir (Ã¶rnek: ÃœskÃ¼dar = Uskudar)
        ilce_mapping = {
            'ÃœskÃ¼dar': 'ÃœskÃ¼dar',   # Standart yazÄ±m
            'Uskudar': 'ÃœskÃ¼dar',   # TÃ¼rkÃ§esiz yazÄ±mÄ± standarda Ã§evir
            'BeÅŸiktaÅŸ': 'BeÅŸiktaÅŸ', # Standart yazÄ±m
            'Besiktas': 'BeÅŸiktaÅŸ', # TÃ¼rkÃ§esiz yazÄ±mÄ± standarda Ã§evir
            'ÅžiÅŸli': 'ÅžiÅŸli',       # Standart yazÄ±m
            'Sisli': 'ÅžiÅŸli'        # TÃ¼rkÃ§esiz yazÄ±mÄ± standarda Ã§evir
        }
        df['ilce'] = df['ilce'].replace(ilce_mapping)  # Mapping'i uygula
        
        # SayÄ±sal deÄŸerlerin makul aralÄ±klarda olduÄŸundan emin ol (aykÄ±rÄ± deÄŸerleri temizle)
        df = df[(df['metrekare'] >= 30) & (df['metrekare'] <= 400)]      # 30-400 mÂ² arasÄ± makul
        df = df[(df['oda_sayisi'] >= 1) & (df['oda_sayisi'] <= 8)]       # 1-8 oda arasÄ± makul
        df = df[(df['yas'] >= 0) & (df['yas'] <= 80)]                    # 0-80 yaÅŸ arasÄ± makul
        df = df[(df['bulundugu_kat'] >= -2) & (df['bulundugu_kat'] <= 40)]  # -2 ile 40. kat arasÄ± makul
        
        # Fiyat Ã¼zerinde daha agresif aykÄ±rÄ± deÄŸer temizleme
        df = df[(df['fiyat'] >= 100000) & (df['fiyat'] <= 50000000)]  # 100bin-50milyon TL arasÄ± makul
        
        # GeliÅŸmiÅŸ aykÄ±rÄ± deÄŸer temizleme - Ã‡ok aÅŸamalÄ± temizleme sistemi
        print(f"Temel temizleme sonrasÄ±: {df.shape[0]} kayÄ±t")
        
        # 1. AÅŸama: Z-score tabanlÄ± aykÄ±rÄ± deÄŸer temizleme (tÃ¼m veri iÃ§in genel temizlik)
        from scipy import stats  # Ä°statistiksel fonksiyonlar iÃ§in
        z_scores = np.abs(stats.zscore(df['fiyat']))  # Her fiyat iÃ§in Z-score hesapla (ortalamadan kaÃ§ sigma uzak)
        df = df[z_scores < 3]  # 3 sigma (standart sapma) dÄ±ÅŸÄ±ndaki deÄŸerleri aykÄ±rÄ± say ve Ã§Ä±kar
        print(f"Z-score temizleme sonrasÄ±: {df.shape[0]} kayÄ±t")
        
        # 2. AÅŸama: Ä°lÃ§e bazÄ±nda daha agresif aykÄ±rÄ± deÄŸer tespiti (her ilÃ§eyi kendi iÃ§inde temizle)
        cleaned_dfs = []  # TemizlenmiÅŸ ilÃ§e verilerini tutacak liste
        for ilce in df['ilce'].unique():  # Her ilÃ§e iÃ§in dÃ¶ngÃ¼
            ilce_df = df[df['ilce'] == ilce].copy()  # Bu ilÃ§enin tÃ¼m verilerini al
            
            if len(ilce_df) < 10:  # Ã‡ok az Ã¶rneÄŸi olan ilÃ§eleri atla (gÃ¼venilir istatistik iÃ§in)
                continue
                
            # Ä°lÃ§e bazÄ±nda fiyat aykÄ±rÄ± deÄŸerleri - IQR yÃ¶ntemiyle agresif temizlik
            Q1_fiyat = ilce_df['fiyat'].quantile(0.1)   # Alt %10 (1. Ã§eyrek yerine daha agresif)
            Q3_fiyat = ilce_df['fiyat'].quantile(0.9)   # Ãœst %10 (3. Ã§eyrek yerine daha agresif)
            IQR_fiyat = Q3_fiyat - Q1_fiyat  # Interquartile Range (Ã§eyrekler arasÄ± fark)
            lower_bound_fiyat = Q1_fiyat - 1.0 * IQR_fiyat  # Alt sÄ±nÄ±r (daha sÄ±kÄ±: 1.0 Ã§arpan)
            upper_bound_fiyat = Q3_fiyat + 1.0 * IQR_fiyat  # Ãœst sÄ±nÄ±r (daha sÄ±kÄ±: 1.0 Ã§arpan)
            # Bu sÄ±nÄ±rlar dÄ±ÅŸÄ±ndaki fiyatlarÄ± Ã§Ä±kar
            ilce_df = ilce_df[(ilce_df['fiyat'] >= lower_bound_fiyat) & (ilce_df['fiyat'] <= upper_bound_fiyat)]
            
            # Metrekare bazÄ±nda da temizleme (her ilÃ§e iÃ§in ayrÄ± ayrÄ±)
            Q1_m2 = ilce_df['metrekare'].quantile(0.05)  # Alt %5
            Q3_m2 = ilce_df['metrekare'].quantile(0.95)  # Ãœst %5
            IQR_m2 = Q3_m2 - Q1_m2  # Metrekare iÃ§in IQR
            lower_bound_m2 = Q1_m2 - 1.5 * IQR_m2  # Alt sÄ±nÄ±r
            upper_bound_m2 = Q3_m2 + 1.5 * IQR_m2  # Ãœst sÄ±nÄ±r
            # Bu sÄ±nÄ±rlar dÄ±ÅŸÄ±ndaki metrekareleri Ã§Ä±kar
            ilce_df = ilce_df[(ilce_df['metrekare'] >= lower_bound_m2) & (ilce_df['metrekare'] <= upper_bound_m2)]
            
            cleaned_dfs.append(ilce_df)  # TemizlenmiÅŸ ilÃ§e verisini listeye ekle
        
        df = pd.concat(cleaned_dfs, ignore_index=True)
        print(f"Ä°lÃ§e bazÄ±nda temizleme sonrasÄ±: {df.shape[0]} kayÄ±t")
        
        # Metrekare baÅŸÄ±na dÃ¼ÅŸen fiyat hesapla ve aykÄ±rÄ± deÄŸerleri temizle
        df['fiyat_metrekare'] = df['fiyat'] / df['metrekare']
        Q1_fiyat_m2 = df['fiyat_metrekare'].quantile(0.01)
        Q3_fiyat_m2 = df['fiyat_metrekare'].quantile(0.99)
        IQR_fiyat_m2 = Q3_fiyat_m2 - Q1_fiyat_m2
        lower_bound_m2 = Q1_fiyat_m2 - 1.5 * IQR_fiyat_m2
        upper_bound_m2 = Q3_fiyat_m2 + 1.5 * IQR_fiyat_m2
        df = df[(df['fiyat_metrekare'] >= lower_bound_m2) & (df['fiyat_metrekare'] <= upper_bound_m2)]
        
        # Ä°lÃ§e bazÄ±nda minimum Ã¶rnek sayÄ±sÄ± kontrolÃ¼
        ilce_counts = df['ilce'].value_counts()
        valid_ilceler = ilce_counts[ilce_counts >= 10].index  # En az 10 Ã¶rneÄŸi olan ilÃ§eleri al
        df = df[df['ilce'].isin(valid_ilceler)]
        
        # Mahalle bazÄ±nda minimum Ã¶rnek sayÄ±sÄ± kontrolÃ¼
        mahalle_counts = df['mahalle'].value_counts()
        valid_mahalleler = mahalle_counts[mahalle_counts >= 5].index  # En az 5 Ã¶rneÄŸi olan mahalleleri al
        df = df[df['mahalle'].isin(valid_mahalleler)]
        
        # Ä°lÃ§e ve mahalle bazÄ±nda fiyat istatistikleri
        ilce_stats = df.groupby('ilce')['fiyat'].agg(['mean', 'median', 'std']).reset_index()
        mahalle_stats = df.groupby('mahalle')['fiyat'].agg(['mean', 'median', 'std']).reset_index()
        
        # Ä°lÃ§e ve mahalle bazÄ±nda fiyat istatistikleri ile encoding
        df = df.merge(ilce_stats, on='ilce', how='left', suffixes=('', '_ilce'))
        df = df.merge(mahalle_stats, on='mahalle', how='left', suffixes=('', '_mahalle'))
        
        # Logaritmik dÃ¶nÃ¼ÅŸÃ¼m (fiyat daÄŸÄ±lÄ±mÄ±nÄ± normalleÅŸtirmek iÃ§in)
        df['fiyat_log'] = np.log1p(df['fiyat'])
        
        # Ä°lÃ§e ve mahalle frekans kodlamasÄ±
        ilce_freq = df['ilce'].value_counts(normalize=True).to_dict()
        mahalle_freq = df['mahalle'].value_counts(normalize=True).to_dict()
        df['ilce_freq'] = df['ilce'].map(ilce_freq)
        df['mahalle_freq'] = df['mahalle'].map(mahalle_freq)
        
        print(f"Veri temizleme sonrasÄ± kalan Ã¶rnek sayÄ±sÄ±: {df.shape[0]}")
        
        return df
    except Exception as e:
        print(f"Veri yÃ¼kleme hatasÄ±: {e}")
        return None

def create_advanced_features(df):
    """Ã–zellik mÃ¼hendisliÄŸi: Polynomial ve complex interactions"""
    # Ham veriden sayÄ±sal Ã¶zellikleri seÃ§ (makine Ã¶ÄŸrenmesi iÃ§in gerekli)
    numerical_cols = ['metrekare', 'oda_sayisi', 'yas', 'bulundugu_kat', 
                     'mean', 'median', 'std', 'mean_mahalle', 'median_mahalle', 'std_mahalle',
                     'ilce_freq', 'mahalle_freq']  # Ä°lÃ§e ve mahalle istatistikleri de dahil
    X_numerical = df[numerical_cols].copy()  # Bu sÃ¼tunlarÄ± kopyala
    
    # KaÃ§ Ã¶zellikle baÅŸladÄ±ÄŸÄ±mÄ±zÄ± kaydet
    print(f"Feature engineering Ã¶ncesi: {X_numerical.shape[1]} Ã¶zellik")
    
    # Temel istatistiksel Ã¶zellikler
    X_numerical.loc[:, 'fiyat_volatilite'] = X_numerical['std'] / (X_numerical['mean'] + 1)  # Volatilite
    X_numerical.loc[:, 'mahalle_volatilite'] = X_numerical['std_mahalle'] / (X_numerical['mean_mahalle'] + 1)
    X_numerical.loc[:, 'mahalle_premium'] = X_numerical['mean_mahalle'] / (X_numerical['mean'] + 1)  # Mahalle primÄ±
    
    # PazarlÄ±k indeksi - fiyat daÄŸÄ±lÄ±mÄ±na dayalÄ±
    X_numerical.loc[:, 'pazarlik_indeksi'] = (X_numerical['mean'] - X_numerical['median']) / (X_numerical['std'] + 1)
    X_numerical.loc[:, 'mahalle_pazarlik_indeksi'] = (X_numerical['mean_mahalle'] - X_numerical['median_mahalle']) / (X_numerical['std_mahalle'] + 1)
    
    # BÃ¶lgesel lÃ¼ks indeksi
    X_numerical.loc[:, 'bolgsel_luksus_skoru'] = X_numerical['mean'] * X_numerical['ilce_freq']
    X_numerical.loc[:, 'mahalle_luksus_skoru'] = X_numerical['mean_mahalle'] * X_numerical['mahalle_freq']
    
    # Temel transformasyonlar - Ä°yileÅŸtirilmiÅŸ
    X_numerical.loc[:, 'metrekare_oda_orani'] = X_numerical['metrekare'] / (X_numerical['oda_sayisi'] + 0.1)
    X_numerical.loc[:, 'metrekare_kare'] = X_numerical['metrekare'] ** 2
    X_numerical.loc[:, 'metrekare_log'] = np.log1p(X_numerical['metrekare'])
    X_numerical.loc[:, 'metrekare_sqrt'] = np.sqrt(X_numerical['metrekare'])
    X_numerical.loc[:, 'metrekare_kup'] = X_numerical['metrekare'] ** 3
    
    # GeliÅŸmiÅŸ metrekare kombinasyonlarÄ±
    X_numerical.loc[:, 'metrekare_oda_kare'] = X_numerical['metrekare_oda_orani'] ** 2
    X_numerical.loc[:, 'metrekare_oda_log'] = np.log1p(X_numerical['metrekare_oda_orani'])
    X_numerical.loc[:, 'ideal_metrekare_sapma'] = np.abs(X_numerical['metrekare'] - (X_numerical['oda_sayisi'] * 25))  # Ä°deal alan sapmasÄ±
    
    # YaÅŸ transformasyonlarÄ± - daha sofistike
    X_numerical.loc[:, 'yas_kare'] = X_numerical['yas'] ** 2
    X_numerical.loc[:, 'yas_log'] = np.log1p(X_numerical['yas'] + 1)
    X_numerical.loc[:, 'yas_sqrt'] = np.sqrt(X_numerical['yas'] + 1)
    X_numerical.loc[:, 'yas_tersi'] = 1 / (X_numerical['yas'] + 1)  # Yeni bina deÄŸeri
    X_numerical.loc[:, 'yas_exp'] = np.exp(-X_numerical['yas'] / 20)  # Yenilik deÄŸeri (exponential decay)
    
    # YaÅŸ kategorileri ve deÄŸer kaybÄ± modeli
    X_numerical.loc[:, 'yeni_bina'] = (X_numerical['yas'] <= 5).astype(int)
    X_numerical.loc[:, 'orta_yas_bina'] = ((X_numerical['yas'] > 5) & (X_numerical['yas'] <= 15)).astype(int)
    X_numerical.loc[:, 'eski_bina'] = (X_numerical['yas'] > 15).astype(int)
    X_numerical.loc[:, 'deger_kaybi_orani'] = np.maximum(0, 1 - (X_numerical['yas'] / 50))  # DeÄŸer kaybÄ± oranÄ±
    
    # Metrekare eficiencysi - geliÅŸmiÅŸ
    X_numerical.loc[:, 'alan_verimliligi_v2'] = X_numerical['metrekare'] / (X_numerical['oda_sayisi'] ** 1.2)
    X_numerical.loc[:, 'oda_buyuklugu_avg'] = X_numerical['metrekare'] / (X_numerical['oda_sayisi'] + 0.5)  # Ortalama oda bÃ¼yÃ¼klÃ¼ÄŸÃ¼
    
    # Oda bÃ¼yÃ¼klÃ¼ÄŸÃ¼ kategorileri
    X_numerical.loc[:, 'genis_odalar'] = (X_numerical['oda_buyuklugu_avg'] > 20).astype(int)
    X_numerical.loc[:, 'orta_odalar'] = ((X_numerical['oda_buyuklugu_avg'] >= 15) & (X_numerical['oda_buyuklugu_avg'] <= 20)).astype(int)
    X_numerical.loc[:, 'dar_odalar'] = (X_numerical['oda_buyuklugu_avg'] < 15).astype(int)
    
    # Kat transformasyonlarÄ±
    X_numerical.loc[:, 'kat_zemin'] = (X_numerical['bulundugu_kat'] == 0).astype(int)
    X_numerical.loc[:, 'kat_yuksek'] = (X_numerical['bulundugu_kat'] > 5).astype(int)
    X_numerical.loc[:, 'kat_bodrum'] = (X_numerical['bulundugu_kat'] < 0).astype(int)
    X_numerical.loc[:, 'kat_1_3'] = ((X_numerical['bulundugu_kat'] >= 1) & (X_numerical['bulundugu_kat'] <= 3)).astype(int)
    X_numerical.loc[:, 'kat_4_7'] = ((X_numerical['bulundugu_kat'] >= 4) & (X_numerical['bulundugu_kat'] <= 7)).astype(int)
    X_numerical.loc[:, 'kat_8_plus'] = (X_numerical['bulundugu_kat'] >= 8).astype(int)
    X_numerical.loc[:, 'kat_log'] = np.log1p(X_numerical['bulundugu_kat'] + 3)
    X_numerical.loc[:, 'kat_kare'] = (X_numerical['bulundugu_kat'] + 3) ** 2
    
    # Kat avantaj skoru
    X_numerical.loc[:, 'kat_avantaj_skoru'] = np.where(
        X_numerical['bulundugu_kat'] < 0, -0.1,  # Bodrum katlar
        np.where(X_numerical['bulundugu_kat'] == 0, 0,  # Zemin kat
        np.where(X_numerical['bulundugu_kat'] <= 3, 0.1,  # DÃ¼ÅŸÃ¼k katlar
        np.where(X_numerical['bulundugu_kat'] <= 7, 0.15,  # Orta katlar
        np.where(X_numerical['bulundugu_kat'] <= 15, 0.05,  # YÃ¼ksek katlar
        -0.05))))  # Ã‡ok yÃ¼ksek katlar
    )
    
    # Kompleks etkileÅŸimler
    X_numerical.loc[:, 'yas_metrekare_etkilesim'] = X_numerical['yas_tersi'] * X_numerical['metrekare_log']
    X_numerical.loc[:, 'kat_alan_etkilesim'] = X_numerical['kat_avantaj_skoru'] * X_numerical['metrekare_oda_orani']
    X_numerical.loc[:, 'premium_lokasyon_skoru'] = X_numerical['bolgsel_luksus_skoru'] * X_numerical['yas_exp']
    
    # Pazar dinamikleri
    X_numerical.loc[:, 'arz_talep_dengesi'] = X_numerical['ilce_freq'] / (X_numerical['mahalle_freq'] + 0.001)
    X_numerical.loc[:, 'fiyat_istikrar_indeksi'] = 1 / (X_numerical['fiyat_volatilite'] + 0.1)
    
    # GeliÅŸmiÅŸ istatistiksel Ã¶zellikler
    X_numerical.loc[:, 'z_score_mahalle'] = (X_numerical['mean_mahalle'] - X_numerical['mean']) / (X_numerical['std'] + 1)
    X_numerical.loc[:, 'mahalle_median_orani'] = X_numerical['median_mahalle'] / (X_numerical['median'] + 1)
    
    # Polynomial Ã¶zellikler
    important_features = ['metrekare', 'oda_sayisi', 'yas_tersi', 'kat_avantaj_skoru']
    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
    poly_features = poly.fit_transform(X_numerical[important_features])
    poly_feature_names = poly.get_feature_names_out(important_features)
    
    # Polynomial Ã¶zellikleri ekle (sadece etkileÅŸim terimleri)
    for i, name in enumerate(poly_feature_names):
        if ' ' in name:  # Sadece etkileÅŸim terimlerini al
            clean_name = name.replace(' ', '_').replace('^', '_pow')
            X_numerical.loc[:, clean_name] = poly_features[:, i]
    
    # NaN ve inf deÄŸerleri temizle
    X_numerical = X_numerical.replace([np.inf, -np.inf], np.nan)
    X_numerical = X_numerical.fillna(X_numerical.median())
    
    print(f"Feature engineering sonrasÄ±: {X_numerical.shape[1]} Ã¶zellik")
    
    return X_numerical

def create_features(df):
    """Ana Ã¶zellik oluÅŸturma fonksiyonu - geriye uyumluluk iÃ§in"""
    return create_advanced_features(df)

def advanced_target_encode_categorical(df, categorical_cols, target_col, n_splits=5):
    """GeliÅŸmiÅŸ target encoding with multiple strategies and smoothing"""
    df_encoded = df.copy()
    
    # StratifiedKFold target encoding iÃ§in
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # Ä°lÃ§e bazÄ±nda stratifikasyon iÃ§in
    ilce_labels = LabelEncoder().fit_transform(df['ilce'])
    
    # Global statistics
    global_mean = df[target_col].mean()
    global_median = df[target_col].median()
    global_std = df[target_col].std()
    
    for col in categorical_cols:
        # Initialize columns
        df_encoded[f'{col}_target_mean'] = 0.0
        df_encoded[f'{col}_target_median'] = 0.0
        df_encoded[f'{col}_target_std'] = 0.0
        df_encoded[f'{col}_target_count'] = 0.0
        df_encoded[f'{col}_target_min'] = 0.0
        df_encoded[f'{col}_target_max'] = 0.0
        df_encoded[f'{col}_target_q25'] = 0.0
        df_encoded[f'{col}_target_q75'] = 0.0
        df_encoded[f'{col}_target_smoothed'] = 0.0  # Bayesian smoothed mean
        
        for train_idx, val_idx in skf.split(df, ilce_labels):
            train_data = df.iloc[train_idx]
            val_data = df.iloc[val_idx]
            
            # Training data'dan geniÅŸletilmiÅŸ istatistikleri hesapla
            target_stats = train_data.groupby(col)[target_col].agg([
                'mean', 'median', 'std', 'count', 'min', 'max',
                lambda x: x.quantile(0.25),  # Q1
                lambda x: x.quantile(0.75)   # Q3
            ]).reset_index()
            
            target_stats.columns = [col, 'mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75']
            
            # Bayesian smoothing (regularization)
            smoothing_factor = 10  # Regularization strength
            target_stats['smoothed_mean'] = (
                (target_stats['count'] * target_stats['mean'] + smoothing_factor * global_mean) / 
                (target_stats['count'] + smoothing_factor)
            )
            
            # NaN handling
            target_stats = target_stats.fillna({
                'mean': global_mean,
                'median': global_median,
                'std': global_std,
                'count': 1,
                'min': global_mean,
                'max': global_mean,
                'q25': global_mean,
                'q75': global_mean,
                'smoothed_mean': global_mean
            })
            
            # Validation data'ya uygula
            val_merged = val_data[[col]].merge(target_stats, on=col, how='left')
            
            # Fill missing values with global statistics
            val_merged = val_merged.fillna({
                'mean': global_mean,
                'median': global_median,
                'std': global_std,
                'count': 1,
                'min': global_mean,
                'max': global_mean,
                'q25': global_mean,
                'q75': global_mean,
                'smoothed_mean': global_mean
            })
            
            # Assign values
            df_encoded.loc[val_idx, f'{col}_target_mean'] = val_merged['mean'].values
            df_encoded.loc[val_idx, f'{col}_target_median'] = val_merged['median'].values
            df_encoded.loc[val_idx, f'{col}_target_std'] = val_merged['std'].values
            df_encoded.loc[val_idx, f'{col}_target_count'] = val_merged['count'].values
            df_encoded.loc[val_idx, f'{col}_target_min'] = val_merged['min'].values
            df_encoded.loc[val_idx, f'{col}_target_max'] = val_merged['max'].values
            df_encoded.loc[val_idx, f'{col}_target_q25'] = val_merged['q25'].values
            df_encoded.loc[val_idx, f'{col}_target_q75'] = val_merged['q75'].values
            df_encoded.loc[val_idx, f'{col}_target_smoothed'] = val_merged['smoothed_mean'].values
    
    return df_encoded

def target_encode_categorical(df, categorical_cols, target_col, n_splits=5):
    """Wrapper for backward compatibility"""
    return advanced_target_encode_categorical(df, categorical_cols, target_col, n_splits)

def train_model(df):
    """GeliÅŸmiÅŸ model eÄŸitimi - XGBoost, LightGBM ve Target Encoding ile ensemble yaklaÅŸÄ±m"""
    try:
        # Tahmin edilecek hedef deÄŸiÅŸkeni belirle (konut fiyatÄ±)
        target_column = 'fiyat'
        y = df[target_column]  # Y deÄŸiÅŸkeni (tahmin edilecek)
        
        # Kategorik deÄŸiÅŸkenler iÃ§in target encoding uygula (ilÃ§e ve mahalle)
        categorical_cols = ['ilce', 'mahalle']  # Bu sÃ¼tunlar kategorik
        # Her ilÃ§e/mahalle iÃ§in ortalama fiyat gibi istatistikleri hesapla
        df_with_target_encoding = target_encode_categorical(df, categorical_cols, target_column)
        
        # GeliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi uygula (yeni Ã¶zellikler tÃ¼ret)
        X_numerical = create_features(df_with_target_encoding)
        
        # Target encoding Ã¶zelliklerini ekle
        target_encoding_cols = [col for col in df_with_target_encoding.columns if 'target_' in col]
        X_target_encoded = df_with_target_encoding[target_encoding_cols]
        
        # One-hot encoding (target encoding ile beraber kullanÄ±m)
        X_categorical = pd.get_dummies(df[categorical_cols], drop_first=True)
        
        # TÃ¼m Ã¶zellikleri birleÅŸtir
        X = pd.concat([X_numerical, X_target_encoded, X_categorical], axis=1)
        
        # Ã–zellik isimlerini sakla
        feature_names = X.columns.tolist()
        
        # Veriyi eÄŸitim ve test setlerine ayÄ±r
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['ilce'])
        
        # Ã–zellikleri Ã¶lÃ§eklendir
        scaler = PowerTransformer(method='yeo-johnson')
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Model listesi
        models = []
        
        # Random Forest modeli (bellek verimli)
        rf_model = RandomForestRegressor(
            n_estimators=200,  # AzaltÄ±ldÄ±
            max_depth=12,      # AzaltÄ±ldÄ±
            min_samples_split=3,
            min_samples_leaf=1,
            max_features='sqrt',
            bootstrap=True,
            oob_score=True,
            random_state=42,
            n_jobs=2  # AzaltÄ±ldÄ±
        )
        models.append(('rf', rf_model))
        

        
        # Gradient Boosting modeli (geliÅŸtirilmiÅŸ)
        gb_model = GradientBoostingRegressor(
            n_estimators=300,
            learning_rate=0.08,
            max_depth=6,
            min_samples_split=3,
            min_samples_leaf=1,
            subsample=0.8,
            max_features='sqrt',
            random_state=42
        )
        models.append(('gb', gb_model))
        

        
        # XGBoost modeli (varsa) - Bellek verimli parametreler
        if XGB_AVAILABLE:
            xgb_model = xgb.XGBRegressor(
                n_estimators=250,           # AzaltÄ±ldÄ±
                max_depth=6,                # AzaltÄ±ldÄ±
                learning_rate=0.08,         # ArtÄ±rÄ±ldÄ± (daha hÄ±zlÄ±)
                subsample=0.8,              # 
                colsample_bytree=0.8,       # 
                reg_alpha=0.1,              # L1 regularization
                reg_lambda=0.1,             # L2 regularization
                min_child_weight=3,         # ArtÄ±rÄ±ldÄ± (overfitting'i azalt)
                gamma=0.1,                  # Minimum split loss
                random_state=42,
                n_jobs=2,                   # AzaltÄ±ldÄ±
                eval_metric='rmse',
                verbosity=0                 # LoglarÄ± azalt
            )
            models.append(('xgb', xgb_model))
            print("XGBoost modeli bellek verimli parametrelerle eklendi")
        
        # LightGBM modeli (varsa) - Bellek verimli
        if LGB_AVAILABLE:
            lgb_model = lgb.LGBMRegressor(
                n_estimators=250,  # AzaltÄ±ldÄ±
                max_depth=6,
                learning_rate=0.08,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=2,  # AzaltÄ±ldÄ±
                verbose=-1
            )
            models.append(('lgb', lgb_model))
        
        # GeliÅŸmiÅŸ model eÄŸitimi - Ä°yileÅŸtirilmiÅŸ ensemble stratejisi
        print(f"GeliÅŸmiÅŸ ensemble eÄŸitimi baÅŸlÄ±yor... ({len(models)} model)")
        
        # YENÄ°: Ã‡ok aÅŸamalÄ± ensemble yaklaÅŸÄ±mÄ±
        # 1. AÅŸama: Bireysel model performanslarÄ±
        individual_scores = {}
        model_predictions = {}
        
        for name, model in models:
            # Stratified K-Fold Cross Validation
            cv_scores = []
            kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            
            # Ä°lÃ§e bazlÄ± stratification iÃ§in discretized target
            y_discrete = pd.qcut(y_train, q=5, labels=False, duplicates='drop')
            
            for train_idx, val_idx in kfold.split(X_train_scaled, y_discrete):
                X_cv_train, X_cv_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
                y_cv_train, y_cv_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
                
                model_copy = type(model)(**model.get_params())
                model_copy.fit(X_cv_train, y_cv_train)
                cv_pred = model_copy.predict(X_cv_val)
                cv_score = r2_score(y_cv_val, cv_pred)
                cv_scores.append(cv_score)
            
            avg_score = np.mean(cv_scores)
            individual_scores[name] = avg_score
            print(f"{name.upper()} CV RÂ² (Î¼Â±Ïƒ): {avg_score:.4f}Â±{np.std(cv_scores):.4f}")
            
            # Final model'i tÃ¼m training data ile eÄŸit
            model.fit(X_train_scaled, y_train)
            model_predictions[name] = model.predict(X_test_scaled)
        
        # 2. AÅŸama: En iyi modelleri seÃ§ (dinamik seÃ§im)
        score_threshold = max(individual_scores.values()) * 0.95  # En iyinin %95'i
        best_models = {name: score for name, score in individual_scores.items() if score >= score_threshold}
        
        if len(best_models) < 2:  # En az 2 model olsun
            sorted_models = sorted(individual_scores.items(), key=lambda x: x[1], reverse=True)
            best_models = dict(sorted_models[:3])  # En iyi 3'Ã¼ al
        
        print(f"SeÃ§ilen modeller ({len(best_models)}): {list(best_models.keys())}")
        print(f"Performans eÅŸiÄŸi: {score_threshold:.4f}")
        
        # 3. AÅŸama: GeliÅŸmiÅŸ aÄŸÄ±rlÄ±klandÄ±rma
        # Performans bazlÄ± aÄŸÄ±rlÄ±k + Ã§eÅŸitlilik bonusu
        base_models_for_ensemble = [(name, model) for name, model in models if name in best_models]
        
        # Ã‡eÅŸitlilik analizi - modeller arasÄ± korelasyon
        pred_matrix = np.column_stack([model_predictions[name] for name in best_models.keys()])
        pred_corr = np.corrcoef(pred_matrix.T)
        
        # AÄŸÄ±rlÄ±k hesaplama: performans * (1 - ortalama korelasyon)
        weights = []
        for i, (name, score) in enumerate(best_models.items()):
            diversity_bonus = 1 - np.mean([pred_corr[i, j] for j in range(len(best_models)) if i != j])
            combined_weight = score * diversity_bonus
            weights.append(combined_weight)
            print(f"{name}: Score={score:.4f}, Diversity={diversity_bonus:.4f}, Weight={combined_weight:.4f}")
        
        # Normalize weights
        weights = np.array(weights)
        weights = weights / weights.sum()
        
        # 4. AÅŸama: Multiple ensemble strategies
        ensemble_strategies = {}
        
        # Strategy 1: AÄŸÄ±rlÄ±klÄ± Average
        weighted_pred = np.average([model_predictions[name] for name in best_models.keys()], 
                                 weights=weights, axis=0)
        ensemble_strategies['weighted_average'] = weighted_pred
        
        # Strategy 2: Dynamic weighting (tahmin bazlÄ±)
        dynamic_pred = np.zeros_like(weighted_pred)
        for i in range(len(weighted_pred)):
            # Her tahmin iÃ§in modellerin ne kadar "emin" olduÄŸunu hesapla
            pred_values = [model_predictions[name][i] for name in best_models.keys()]
            pred_std = np.std(pred_values)
            
            if pred_std < np.mean([model_predictions[name].std() for name in best_models.keys()]) * 0.5:
                # DÃ¼ÅŸÃ¼k belirsizlik - performans aÄŸÄ±rlÄ±ÄŸÄ± kullan
                dynamic_weights = weights
            else:
                # YÃ¼ksek belirsizlik - eÅŸit aÄŸÄ±rlÄ±k kullan
                dynamic_weights = np.ones(len(weights)) / len(weights)
            
            dynamic_pred[i] = np.average(pred_values, weights=dynamic_weights)
        
        ensemble_strategies['dynamic_weighting'] = dynamic_pred
        
        # Strategy 3: Meta-learner (Ridge)
        if len(best_models) >= 2:
            meta_features = np.column_stack([model_predictions[name] for name in best_models.keys()])
            meta_learner = Ridge(alpha=1.0, random_state=42)
            meta_learner.fit(meta_features, y_test)
            meta_pred = meta_learner.predict(meta_features)
            ensemble_strategies['meta_learner'] = meta_pred
        
        # 5. AÅŸama: En iyi ensemble stratejisini seÃ§
        best_strategy = None
        best_score = -np.inf
        
        for strategy_name, predictions in ensemble_strategies.items():
            r2 = r2_score(y_test, predictions)
            mape = mean_absolute_percentage_error(y_test, predictions) * 100
            print(f"{strategy_name}: RÂ²={r2:.4f}, MAPE={mape:.2f}%")
            
            if r2 > best_score:
                best_score = r2
                best_strategy = strategy_name
                y_pred = predictions
        
        print(f"En iyi ensemble stratejisi: {best_strategy} (RÂ²={best_score:.4f})")
        
        # 6. AÅŸama: Ensemble model objesi oluÅŸtur (kaydetmek iÃ§in)
        selected_model_list = [(name, model) for name, model in models if name in best_models]
        
        if best_strategy == 'meta_learner' and len(selected_model_list) >= 2:
            ensemble_model = StackingRegressor(
                estimators=selected_model_list,
                final_estimator=Ridge(alpha=1.0, random_state=42),
                cv=3,
                n_jobs=2
            )
            ensemble_model.fit(X_train_scaled, y_train)
        else:
            # Weighted voting regressor
            ensemble_model = VotingRegressor(
                estimators=selected_model_list, 
                weights=weights
            )
            ensemble_model.fit(X_train_scaled, y_train)
        
        # TÃ¼m Ã¶zellikleri kullanÄ±yoruz
        selected_features = feature_names
        
        # YENÄ°: GeliÅŸmiÅŸ gÃ¼ven aralÄ±ÄŸÄ± hesaplama
        # Model belirsizliÄŸini tahmin etmek iÃ§in bootstrap sampling
        print("GÃ¼ven aralÄ±ÄŸÄ± hesaplamasÄ± iÃ§in bootstrap analizi...")
        n_bootstrap = 10  # 50'den 10'a dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - hÄ±zlandÄ±rma
        bootstrap_predictions = []
        
        # Sadece en iyi 2 modeli kullan (daha hÄ±zlÄ±)
        top_2_models = list(best_models.keys())[:2]
        print(f"Bootstrap iÃ§in sadece en iyi 2 model kullanÄ±lÄ±yor: {top_2_models}")
        
        for i in range(n_bootstrap):
            print(f"Bootstrap {i+1}/{n_bootstrap}...")
            # Bootstrap sample oluÅŸtur
            bootstrap_indices = np.random.choice(len(X_train_scaled), size=len(X_train_scaled)//2, replace=True)  # YarÄ± boyut
            X_bootstrap = X_train_scaled[bootstrap_indices]
            y_bootstrap = y_train.iloc[bootstrap_indices]
            
            # Sadece en iyi 2 modeli bootstrap verisi ile eÄŸit
            bootstrap_preds = []
            for name in top_2_models:
                model_for_bootstrap = type([m for n, m in models if n == name][0])(**[m for n, m in models if n == name][0].get_params())
                model_for_bootstrap.fit(X_bootstrap, y_bootstrap)
                bootstrap_pred = model_for_bootstrap.predict(X_test_scaled)
                bootstrap_preds.append(bootstrap_pred)
            
            # Ensemble prediction (sadece top 2 modelin aÄŸÄ±rlÄ±klarÄ±)
            top_2_weights = weights[:2] / weights[:2].sum()  # Normalize
            ensemble_bootstrap_pred = np.average(bootstrap_preds, weights=top_2_weights, axis=0)
            bootstrap_predictions.append(ensemble_bootstrap_pred)
        
        bootstrap_predictions = np.array(bootstrap_predictions)
        
        # Bootstrap tabanlÄ± gÃ¼ven aralÄ±klarÄ±
        confidence_lower = np.percentile(bootstrap_predictions, 16, axis=0)  # %68 CI alt sÄ±nÄ±r
        confidence_upper = np.percentile(bootstrap_predictions, 84, axis=0)  # %68 CI Ã¼st sÄ±nÄ±r
        
        # Ortalama tahmin belirsizliÄŸi
        prediction_std = np.std(bootstrap_predictions, axis=0)
        mean_uncertainty = np.mean(prediction_std)
        
        print(f"Ortalama tahmin belirsizliÄŸi: Â±{mean_uncertainty:,.0f} TL")
        
        # GÃ¼ven aralÄ±ÄŸÄ± geniÅŸliÄŸi analizi
        ci_width = confidence_upper - confidence_lower
        print(f"Ortalama gÃ¼ven aralÄ±ÄŸÄ± geniÅŸliÄŸi: {np.mean(ci_width):,.0f} TL")
        
        # GeliÅŸmiÅŸ model performans deÄŸerlendirmesi
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        
        # Ortalama mutlak yÃ¼zde hata (MAPE)
        mape = mean_absolute_percentage_error(y_test, y_pred) * 100
        
        # GeliÅŸmiÅŸ metrikler
        # Median Absolute Error
        median_ae = np.median(np.abs(y_test - y_pred))
        
        # R2 adjusted (feature sayÄ±sÄ±nÄ± hesaba katan)
        n = len(y_test)
        p = X_test.shape[1]
        r2_adj = 1 - (1 - r2) * (n - 1) / (n - p - 1)
        
        # Fiyat aralÄ±ÄŸÄ±na gÃ¶re performans
        price_ranges = [
            (0, 1000000, "1M altÄ±"),
            (1000000, 3000000, "1M-3M arasÄ±"),  
            (3000000, 5000000, "3M-5M arasÄ±"),
            (5000000, float('inf'), "5M Ã¼stÃ¼")
        ]
        
        print(f"\n=== GELÄ°ÅžMÄ°Åž MODEL PERFORMANSI ===")
        print(f"MSE: {mse:,.0f}")
        print(f"RMSE: {rmse:,.0f}")
        print(f"MAE: {mae:,.0f}")
        print(f"Median AE: {median_ae:,.0f}")
        print(f"RÂ² Skoru: {r2:.4f}")
        print(f"RÂ² Adjusted: {r2_adj:.4f}")
        print(f"MAPE: %{mape:.2f}")
        
        # Fiyat aralÄ±ÄŸÄ±na gÃ¶re performans analizi
        print(f"\n=== FÄ°YAT ARALIÄžINA GÃ–RE PERFORMANS ===")
        for min_price, max_price, label in price_ranges:
            mask = (y_test >= min_price) & (y_test < max_price)
            if mask.sum() > 0:
                range_r2 = r2_score(y_test[mask], y_pred[mask])
                range_mape = mean_absolute_percentage_error(y_test[mask], y_pred[mask]) * 100
                print(f"{label}: RÂ²={range_r2:.3f}, MAPE=%{range_mape:.1f}, n={mask.sum()}")
        
        # Residual analizi
        residuals = y_test - y_pred
        residual_std = np.std(residuals)
        print(f"\n=== RESIDUAL ANALÄ°ZÄ° ===")
        print(f"Residual std: {residual_std:,.0f}")
        print(f"Residual mean: {np.mean(residuals):,.0f}")
        print(f"% tahminler %10 hata bandÄ±nda: {(np.abs(residuals/y_test) < 0.1).mean()*100:.1f}%")
        print(f"% tahminler %20 hata bandÄ±nda: {(np.abs(residuals/y_test) < 0.2).mean()*100:.1f}%")
        
        # Bireysel model performanslarÄ±nÄ± gÃ¶ster
        print(f"\nBireysel Model PerformanslarÄ±:")
        if hasattr(ensemble_model, 'named_estimators_'):
            for name, model in ensemble_model.named_estimators_.items():
                if hasattr(model, 'predict'):
                    individual_pred = model.predict(X_test_scaled)
                    individual_r2 = r2_score(y_test, individual_pred)
                    individual_mape = mean_absolute_percentage_error(y_test, individual_pred) * 100
                    print(f"{name.upper()}: RÂ² = {individual_r2:.4f}, MAPE = %{individual_mape:.2f}")
                    
                    # OOB skorunu gÃ¶ster (varsa)
                    if hasattr(model, 'oob_score_'):
                        print(f"  OOB Skoru: {model.oob_score_:.4f}")
        else:
            print("Stacking ensemble kullanÄ±ldÄ±ÄŸÄ± iÃ§in bireysel skorlar base modeller iÃ§in gÃ¶sterilemiyor.")
        
        # Ã‡apraz doÄŸrulama kaldÄ±rÄ±ldÄ± - hÄ±zlÄ± eÄŸitim iÃ§in
        # Zaten train/test split ile performans Ã¶lÃ§Ã¼lÃ¼yor
        print("\nÃ‡apraz doÄŸrulama atlandÄ± (hÄ±zlandÄ±rma iÃ§in)")
        print("Train/Test split ile performans Ã¶lÃ§Ã¼mÃ¼ yeterli.")
        
        # En Ã¶nemli Ã¶zellikleri gÃ¶ster
        print("\nEn Ã¶nemli 15 Ã¶zellik:")
        if hasattr(ensemble_model, 'named_estimators_') and 'rf' in ensemble_model.named_estimators_:
            rf_model = ensemble_model.named_estimators_['rf']
            feature_importances = pd.Series(rf_model.feature_importances_, index=feature_names).sort_values(ascending=False)
            print(feature_importances.head(15))
        else:
            print("Stacking ensemble kullanÄ±ldÄ±ÄŸÄ± iÃ§in feature importances gÃ¶sterilemiyor.")
        
        # Ã–zellik Ã¶nem grafiÄŸi
        plt.figure(figsize=(10, 6))
        top_features = feature_importances.head(15)
        sns.barplot(x=top_features.values, y=top_features.index)
        plt.title('En Ã–nemli 15 Ã–zellik')
        plt.tight_layout()
        
        # Grafikleri kaydet
        if not os.path.exists('plots'):
            os.makedirs('plots')
        plt.savefig('plots/feature_importance.png')
        
        # GerÃ§ek vs Tahmin grafiÄŸi
        plt.figure(figsize=(10, 6))
        plt.scatter(y_test, y_pred, alpha=0.5)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
        plt.xlabel('GerÃ§ek Fiyat')
        plt.ylabel('Tahmin Edilen Fiyat')
        plt.title('GerÃ§ek vs Tahmin')
        plt.tight_layout()
        plt.savefig('plots/actual_vs_predicted.png')
        
        # Modeli ve scaler'Ä± kaydet
        if not os.path.exists('models'):
            os.makedirs('models')
        
        print("Model ve ilgili dosyalar kaydediliyor...")
        joblib.dump(ensemble_model, 'models/konut_fiyat_model.pkl')
        joblib.dump(scaler, 'models/scaler.pkl')
        joblib.dump(feature_names, 'models/feature_names.pkl')  # TÃ¼m Ã¶zellikler
        
        # YENÄ°: Bootstrap ve gÃ¼ven aralÄ±ÄŸÄ± parametreleri
        confidence_params = {
            'bootstrap_predictions': bootstrap_predictions,
            'confidence_lower': confidence_lower,
            'confidence_upper': confidence_upper,
            'prediction_std': prediction_std,
            'mean_uncertainty': mean_uncertainty,
            'weights': weights,
            'best_models': list(best_models.keys()),
            'best_strategy': best_strategy,
            'ensemble_type': type(ensemble_model).__name__
        }
        joblib.dump(confidence_params, 'models/confidence_params.pkl')
        
        # Model performans metrikleri
        performance_metrics = {
            'r2_score': r2,
            'rmse': rmse,
            'mae': mae,
            'mape': mape,
            'median_ae': median_ae,
            'r2_adjusted': r2_adj,
            'residual_std': residual_std,
            'accuracy_10_percent': (np.abs(residuals/y_test) < 0.1).mean(),
            'accuracy_20_percent': (np.abs(residuals/y_test) < 0.2).mean(),
            'training_date': pd.Timestamp.now().isoformat(),
            'n_training_samples': len(X_train),
            'n_test_samples': len(X_test),
            'n_features': len(feature_names)
        }
        joblib.dump(performance_metrics, 'models/performance_metrics.pkl')
        
        # Ä°lÃ§e-mahalle haritasÄ± oluÅŸtur
        ilce_mahalle_map = {}
        for ilce in df['ilce'].unique():
            ilce_mahalle_map[ilce] = sorted(df[df['ilce'] == ilce]['mahalle'].unique().tolist())
        
        joblib.dump(sorted(df['ilce'].unique().tolist()), 'models/unique_ilce.pkl')
        joblib.dump(ilce_mahalle_map, 'models/ilce_mahalle_map.pkl')
        
        # Tahmin gÃ¼venilirliÄŸi iÃ§in fiyat aralÄ±klarÄ±nÄ± kaydet
        price_range = {
            'min': float(y.min()),
            'max': float(y.max()),
            'mean': float(y.mean()),
            'median': float(y.median()),
            'q1': float(y.quantile(0.25)),
            'q3': float(y.quantile(0.75)),
            'q5': float(y.quantile(0.05)),
            'q95': float(y.quantile(0.95))
        }
        joblib.dump(price_range, 'models/price_range.pkl')
        
        print("âœ… Model baÅŸarÄ±yla kaydedildi!")
        print(f"ðŸ“Š Final Performans: RÂ²={r2:.4f}, RMSE={rmse:,.0f}, MAPE={mape:.2f}%")
        
        return ensemble_model, scaler, feature_names
    except Exception as e:
        print(f"Model eÄŸitimi hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def predict_price(features_dict):
    """Konut fiyatÄ±nÄ± tahmin et ve gÃ¼venilirlik bilgisi dÃ¶ndÃ¼r"""
    try:
        # Modeli ve ilgili dosyalarÄ± yÃ¼kle
        model = joblib.load('models/konut_fiyat_model.pkl')
        scaler = joblib.load('models/scaler.pkl')
        feature_names = joblib.load('models/feature_names.pkl')
        price_range = joblib.load('models/price_range.pkl')
        
        # Feature selector artÄ±k kullanÄ±lmÄ±yor - hÄ±zlandÄ±rma iÃ§in kaldÄ±rÄ±ldÄ±
        
        # Veri setini yÃ¼kle
        df = load_and_preprocess_data()
        
        # Ä°lÃ§e ve mahalle istatistikleri
        ilce_stats = df.groupby('ilce')['fiyat'].agg(['mean', 'median', 'std']).reset_index()
        mahalle_stats = df.groupby('mahalle')['fiyat'].agg(['mean', 'median', 'std']).reset_index()
        
        # Ä°lÃ§e ve mahalle frekans kodlamasÄ±
        ilce_freq = df['ilce'].value_counts(normalize=True).to_dict()
        mahalle_freq = df['mahalle'].value_counts(normalize=True).to_dict()
        
        # Girdi Ã¶zelliklerini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
        input_df = pd.DataFrame([features_dict])
        
        # Ä°lÃ§e ve mahalle istatistiklerini ekle
        input_df = input_df.merge(ilce_stats, on='ilce', how='left')
        input_df = input_df.merge(mahalle_stats, on='mahalle', how='left', suffixes=('', '_mahalle'))
        
        # Frekans kodlamasÄ±nÄ± ekle
        input_df['ilce_freq'] = input_df['ilce'].map(ilce_freq)
        input_df['mahalle_freq'] = input_df['mahalle'].map(mahalle_freq)
        
        # Target encoding Ã¶zelliklerini ekle
        # Global istatistiklerle doldur (tahmin aÅŸamasÄ±nda cross-validation yapmayÄ±z)
        global_mean = df['fiyat'].mean()
        global_median = df['fiyat'].median()
        global_std = df['fiyat'].std()
        
        # GeliÅŸmiÅŸ Ä°lÃ§e target encoding
        ilce_target_stats = df.groupby('ilce')['fiyat'].agg([
            'mean', 'median', 'std', 'count', 'min', 'max',
            lambda x: x.quantile(0.25),  # Q1
            lambda x: x.quantile(0.75)   # Q3
        ]).reset_index()
        ilce_target_stats.columns = ['ilce', 'mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75']
        
        # Smoothed mean hesapla
        smoothing_factor = 10
        ilce_target_stats['smoothed_mean'] = (
            (ilce_target_stats['count'] * ilce_target_stats['mean'] + smoothing_factor * global_mean) / 
            (ilce_target_stats['count'] + smoothing_factor)
        )
        
        # Basit ve gÃ¼venli target encoding
        # Ä°lÃ§e iÃ§in
        try:
            temp_ilce = input_df.merge(ilce_target_stats, on='ilce', how='left')
            for col in ['mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75', 'smoothed_mean']:
                if col in temp_ilce.columns:
                    default_val = global_mean if col in ['mean', 'median', 'min', 'max', 'q25', 'q75', 'smoothed_mean'] else (global_std if col == 'std' else 1)
                    input_df[f'ilce_target_{col}'] = temp_ilce[col].fillna(default_val)
                else:
                    default_val = global_mean if col in ['mean', 'median', 'min', 'max', 'q25', 'q75', 'smoothed_mean'] else (global_std if col == 'std' else 1)
                    input_df[f'ilce_target_{col}'] = default_val
        except:
            # EÄŸer merge baÅŸarÄ±sÄ±zsa varsayÄ±lan deÄŸerler kullan
            for col in ['mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75', 'smoothed_mean']:
                default_val = global_mean if col in ['mean', 'median', 'min', 'max', 'q25', 'q75', 'smoothed_mean'] else (global_std if col == 'std' else 1)
                input_df[f'ilce_target_{col}'] = default_val
        
        # Mahalle iÃ§in  
        try:
            # Mahalle istatistiklerini hesapla
            mahalle_target_stats = df.groupby('mahalle')['fiyat'].agg([
                'mean', 'median', 'std', 'count', 'min', 'max',
                lambda x: x.quantile(0.25),  # Q1
                lambda x: x.quantile(0.75)   # Q3
            ]).reset_index()
            mahalle_target_stats.columns = ['mahalle', 'mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75']
            
            # Smoothed mean hesapla
            mahalle_target_stats['smoothed_mean'] = (
                (mahalle_target_stats['count'] * mahalle_target_stats['mean'] + smoothing_factor * global_mean) / 
                (mahalle_target_stats['count'] + smoothing_factor)
            )
            
            temp_mahalle = input_df.merge(mahalle_target_stats, on='mahalle', how='left')
            for col in ['mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75', 'smoothed_mean']:
                if col in temp_mahalle.columns:
                    default_val = global_mean if col in ['mean', 'median', 'min', 'max', 'q25', 'q75', 'smoothed_mean'] else (global_std if col == 'std' else 1)
                    input_df[f'mahalle_target_{col}'] = temp_mahalle[col].fillna(default_val)
                else:
                    default_val = global_mean if col in ['mean', 'median', 'min', 'max', 'q25', 'q75', 'smoothed_mean'] else (global_std if col == 'std' else 1)
                    input_df[f'mahalle_target_{col}'] = default_val
        except:
            # EÄŸer merge baÅŸarÄ±sÄ±zsa varsayÄ±lan deÄŸerler kullan
            for col in ['mean', 'median', 'std', 'count', 'min', 'max', 'q25', 'q75', 'smoothed_mean']:
                default_val = global_mean if col in ['mean', 'median', 'min', 'max', 'q25', 'q75', 'smoothed_mean'] else (global_std if col == 'std' else 1)
                input_df[f'mahalle_target_{col}'] = default_val
        
        # Ã–zellik mÃ¼hendisliÄŸi
        X_numerical = create_features(input_df)
        
        # GeliÅŸmiÅŸ target encoding Ã¶zelliklerini al
        target_encoding_cols = [
            'ilce_target_mean', 'ilce_target_median', 'ilce_target_std',
            'ilce_target_count', 'ilce_target_min', 'ilce_target_max', 
            'ilce_target_q25', 'ilce_target_q75', 'ilce_target_smoothed',
            'mahalle_target_mean', 'mahalle_target_median', 'mahalle_target_std',
            'mahalle_target_count', 'mahalle_target_min', 'mahalle_target_max',
            'mahalle_target_q25', 'mahalle_target_q75', 'mahalle_target_smoothed'
        ]
        
        # Eksik sÃ¼tunlarÄ± 0 ile doldur (eski model uyumluluÄŸu iÃ§in)
        for col in target_encoding_cols:
            if col not in input_df.columns:
                input_df[col] = global_mean if 'mean' in col or 'median' in col or 'smoothed' in col else global_std if 'std' in col else 1 if 'count' in col else global_mean
        
        X_target_encoded = input_df[target_encoding_cols]
        
        # Kategorik Ã¶zellikleri one-hot encoding ile dÃ¶nÃ¼ÅŸtÃ¼r
        categorical_cols = ['ilce', 'mahalle']
        X_categorical = pd.get_dummies(input_df[categorical_cols], drop_first=True)
        
        # TÃ¼m Ã¶zellikleri birleÅŸtir
        X_combined = pd.concat([X_numerical, X_target_encoded, X_categorical], axis=1)
        
        # One-hot encoding'te eksik olan sÃ¼tunlarÄ± 0 olarak doldurmak iÃ§in boÅŸ bir DataFrame oluÅŸtur
        missing_cols = set(feature_names) - set(X_combined.columns)
        missing_df = pd.DataFrame(0, index=X_combined.index, columns=list(missing_cols))
        
        # TÃ¼m Ã¶zellikleri birleÅŸtir
        X = pd.concat([X_combined, missing_df], axis=1)
        
        # Ã–zellikleri model iÃ§in sÄ±rala
        X = X.reindex(columns=feature_names, fill_value=0)
        
        # Ã–zellikleri Ã¶lÃ§eklendir
        X_scaled = scaler.transform(X)
        
        # Tahmin yap
        prediction = model.predict(X_scaled)[0]
        
        # Negatif fiyatlarÄ± dÃ¼zelt
        prediction = max(0, prediction)
        
        # YENÄ°: Bootstrap tabanlÄ± gÃ¼ven aralÄ±ÄŸÄ± hesaplama (eÄŸer varsa)
        try:
            confidence_params = joblib.load('models/confidence_params.pkl')
            bootstrap_predictions = confidence_params['bootstrap_predictions']
            prediction_std = confidence_params['prediction_std']
            mean_uncertainty = confidence_params['mean_uncertainty']
            
            # Tek bir tahmin iÃ§in belirsizlik tahmini
            # Ortalama belirsizliÄŸi kullanarak gÃ¼ven aralÄ±ÄŸÄ± hesapla
            single_prediction_uncertainty = mean_uncertainty
            
            # %68 gÃ¼ven aralÄ±ÄŸÄ± (1 sigma)
            lower_bound = max(0, prediction - single_prediction_uncertainty)
            upper_bound = prediction + single_prediction_uncertainty
            
            # %95 gÃ¼ven aralÄ±ÄŸÄ± (2 sigma) - daha geniÅŸ
            lower_bound_95 = max(0, prediction - 2 * single_prediction_uncertainty)
            upper_bound_95 = prediction + 2 * single_prediction_uncertainty
            
            confidence_interval = single_prediction_uncertainty
            reliability_score = 1.0 - (single_prediction_uncertainty / prediction) if prediction > 0 else 0.5
            
        except FileNotFoundError:
            # Bootstrap verileri yoksa eski yÃ¶ntemi kullan
            confidence_interval = prediction * 0.15
            lower_bound = max(0, prediction - confidence_interval)
            upper_bound = prediction + confidence_interval
            lower_bound_95 = max(0, prediction - 2 * confidence_interval)
            upper_bound_95 = prediction + 2 * confidence_interval
            reliability_score = 0.8
        
        # Tahmin gÃ¼venilirliÄŸini deÄŸerlendir
        reliability = "YÃ¼ksek"
        warning = None
        
        # Fiyat aralÄ±ÄŸÄ± dÄ±ÅŸÄ±nda mÄ± kontrol et
        if prediction < price_range['q5'] or prediction > price_range['q95']:
            reliability = "DÃ¼ÅŸÃ¼k"
            reliability_score = max(0.2, reliability_score * 0.4)
            warning = "Tahmin edilen fiyat normal fiyat aralÄ±ÄŸÄ±nÄ±n dÄ±ÅŸÄ±nda."
        elif prediction < price_range['q1'] * 0.8 or prediction > price_range['q3'] * 1.2:
            reliability = "Orta"
            reliability_score = max(0.5, reliability_score * 0.7)
            warning = "Tahmin edilen fiyat normal fiyat aralÄ±ÄŸÄ±nÄ±n sÄ±nÄ±rlarÄ±nda."
        
        result = {
            'prediction': prediction,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'lower_bound_95': lower_bound_95,
            'upper_bound_95': upper_bound_95,
            'confidence_interval': confidence_interval,
            'reliability': reliability,
            'reliability_score': reliability_score,
            'warning': warning,
            'price_per_m2': prediction / features_dict['metrekare'],
            'prediction_quality': 'YÃ¼ksek' if reliability_score > 0.8 else 'Orta' if reliability_score > 0.5 else 'DÃ¼ÅŸÃ¼k'
        }
        
        return result
    except Exception as e:
        print(f"Tahmin hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_available_features():
    """KullanÄ±labilir Ã¶zellikleri dÃ¶ndÃ¼r"""
    try:
        if os.path.exists('models/unique_ilce.pkl'):
            unique_ilce = joblib.load('models/unique_ilce.pkl')
            
            # Ä°lÃ§e-mahalle iliÅŸkisini kontrol et
            if os.path.exists('models/ilce_mahalle_map.pkl'):
                ilce_mahalle_map = joblib.load('models/ilce_mahalle_map.pkl')
            else:
                # Model kaydedilmiÅŸ ama ilÃ§e-mahalle haritasÄ± yoksa yeniden oluÅŸtur
                df = load_and_preprocess_data()
                ilce_mahalle_map = {}
                for ilce in df['ilce'].unique():
                    ilce_mahalle_map[ilce] = sorted(df[df['ilce'] == ilce]['mahalle'].unique().tolist())
                
                # HaritayÄ± kaydet
                joblib.dump(ilce_mahalle_map, 'models/ilce_mahalle_map.pkl')
            
            return {
                'ilce': unique_ilce,
                'ilce_mahalle_map': ilce_mahalle_map
            }
        else:
            # Model henÃ¼z eÄŸitilmemiÅŸse veri setinden al
            df = load_and_preprocess_data()
            if df is not None:
                # Ä°lÃ§e-mahalle haritasÄ±nÄ± oluÅŸtur
                ilce_mahalle_map = {}
                for ilce in df['ilce'].unique():
                    ilce_mahalle_map[ilce] = sorted(df[df['ilce'] == ilce]['mahalle'].unique().tolist())
                
                return {
                    'ilce': sorted(df['ilce'].unique().tolist()),
                    'ilce_mahalle_map': ilce_mahalle_map
                }
            return None
    except Exception as e:
        print(f"Ã–zellik bilgilerini alma hatasÄ±: {e}")
        return None

def get_district_stats():
    """Ä°lÃ§e bazÄ±nda fiyat istatistiklerini dÃ¶ndÃ¼rÃ¼r"""
    try:
        # Veri setini yÃ¼kle
        df = load_and_preprocess_data()
        
        if df is not None:
            # Ä°lÃ§e bazÄ±nda ortalama, medyan, minimum ve maksimum fiyatlarÄ± hesapla
            district_stats = df.groupby('ilce')['fiyat'].agg(['mean', 'median', 'min', 'max', 'count', 'std']).reset_index()
            district_stats = district_stats.sort_values('mean', ascending=False)
            
            # Metrekare baÅŸÄ±na ortalama fiyatÄ± hesapla
            df['fiyat_metrekare'] = df['fiyat'] / df['metrekare']
            price_per_sqm = df.groupby('ilce')['fiyat_metrekare'].mean().reset_index()
            
            # Ä°lÃ§e istatistiklerine metrekare baÅŸÄ±na fiyatÄ± ekle
            district_stats = district_stats.merge(price_per_sqm, on='ilce')
            
            # SonuÃ§larÄ± sÃ¶zlÃ¼k olarak dÃ¶ndÃ¼r
            result = {}
            for _, row in district_stats.iterrows():
                result[row['ilce']] = {
                    'mean': float(row['mean']),
                    'median': float(row['median']),
                    'min': float(row['min']),
                    'max': float(row['max']),
                    'count': int(row['count']),
                    'std': float(row['std']),
                    'price_per_sqm': float(row['fiyat_metrekare'])
                }
            
            return result
        return None
    except Exception as e:
        print(f"Ä°lÃ§e istatistikleri hesaplanÄ±rken hata oluÅŸtu: {e}")
        return None

def get_price_range_performance():
    """Fiyat aralÄ±ÄŸÄ±na gÃ¶re model performansÄ±nÄ± dÃ¶ndÃ¼rÃ¼r"""
    try:
        # Model dosyalarÄ±nÄ± kontrol et
        if not os.path.exists('models/konut_fiyat_model.pkl'):
            print("âŒ EÄŸitilmiÅŸ model bulunamadÄ±!")
            return None
        
        # Modeli ve scaler'Ä± yÃ¼kle
        model = joblib.load('models/konut_fiyat_model.pkl')
        scaler = joblib.load('models/scaler.pkl')
        feature_names = joblib.load('models/feature_names.pkl')
        
        # Veri setini yÃ¼kle ve iÅŸle
        df = load_and_preprocess_data()
        if df is None:
            return None
        
        # Hedef deÄŸiÅŸken
        target_column = 'fiyat'
        y = df[target_column]
        
        # Target encoding uygula
        categorical_cols = ['ilce', 'mahalle']
        df_with_target_encoding = target_encode_categorical(df, categorical_cols, target_column)
        
        # Ã–zellik mÃ¼hendisliÄŸi
        X_numerical = create_features(df_with_target_encoding)
        
        # Target encoding Ã¶zelliklerini ekle
        target_encoding_cols = [col for col in df_with_target_encoding.columns if 'target_' in col]
        X_target_encoded = df_with_target_encoding[target_encoding_cols]
        
        # One-hot encoding
        X_categorical = pd.get_dummies(df[categorical_cols], drop_first=True)
        
        # TÃ¼m Ã¶zellikleri birleÅŸtir
        X = pd.concat([X_numerical, X_target_encoded, X_categorical], axis=1)
        
        # Ã–zellikleri model iÃ§in sÄ±rala
        X = X.reindex(columns=feature_names, fill_value=0)
        
        # Test verisi oluÅŸtur (son %20)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['ilce'])
        
        # Ã–zellikleri Ã¶lÃ§eklendir
        X_test_scaled = scaler.transform(X_test)
        
        # Tahmin yap
        y_pred = model.predict(X_test_scaled)
        
        # Fiyat aralÄ±klarÄ± tanÄ±mla
        price_ranges = [
            (0, 1000000, "1M altÄ±"),
            (1000000, 2000000, "1M-2M arasÄ±"),  
            (2000000, 3000000, "2M-3M arasÄ±"),
            (3000000, 5000000, "3M-5M arasÄ±"),
            (5000000, float('inf'), "5M Ã¼stÃ¼")
        ]
        
        performance_results = {}
        
        print(f"\nðŸŽ¯ FÄ°YAT ARALIÄžINA GÃ–RE MODEL PERFORMANSI")
        print("=" * 60)
        
        for min_price, max_price, label in price_ranges:
            # Bu fiyat aralÄ±ÄŸÄ±ndaki veriyi filtrele
            mask = (y_test >= min_price) & (y_test < max_price)
            
            if mask.sum() > 5:  # En az 5 Ã¶rnek olmalÄ±
                y_test_range = y_test[mask]
                y_pred_range = y_pred[mask]
                
                # Performans metrikleri hesapla
                r2 = r2_score(y_test_range, y_pred_range)
                mse = mean_squared_error(y_test_range, y_pred_range)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(y_test_range, y_pred_range)
                mape = mean_absolute_percentage_error(y_test_range, y_pred_range) * 100
                
                # Ortalama fiyat
                avg_price = y_test_range.mean()
                
                performance_results[label] = {
                    'r2_score': float(r2),
                    'rmse': float(rmse),
                    'mae': float(mae),
                    'mape': float(mape),
                    'sample_count': int(mask.sum()),
                    'avg_price': float(avg_price),
                    'min_price': float(min_price),
                    'max_price': float(max_price) if max_price != float('inf') else None
                }
                
                # SonuÃ§larÄ± yazdÄ±r
                print(f"\nðŸ“Š {label}:")
                print(f"   â€¢ RÂ² Skoru: {r2:.4f}")
                print(f"   â€¢ RMSE: {rmse:,.0f} TL")
                print(f"   â€¢ MAE: {mae:,.0f} TL") 
                print(f"   â€¢ MAPE: %{mape:.2f}")
                print(f"   â€¢ Ã–rnek SayÄ±sÄ±: {mask.sum()}")
                print(f"   â€¢ Ortalama Fiyat: {avg_price:,.0f} TL")
                
                # Performans yorumu
                if r2 >= 0.9:
                    performance = "ðŸŸ¢ MÃ¼kemmel"
                elif r2 >= 0.8:
                    performance = "ðŸŸ¡ Ä°yi"
                elif r2 >= 0.7:
                    performance = "ðŸŸ  Orta"
                elif r2 >= 0.5:
                    performance = "ðŸŸ  DÃ¼ÅŸÃ¼k"
                elif r2 >= 0.0:
                    performance = "ðŸ”´ ZayÄ±f"
                else:
                    performance = "ðŸ”´ Ã‡ok ZayÄ±f (Negatif RÂ²)"
                    
                print(f"   â€¢ Performans: {performance}")
                
                # RÂ² negatifse aÃ§Ä±klama ekle
                if r2 < 0:
                    print(f"   âš ï¸  Negatif RÂ² = Model rastgele tahminden daha kÃ¶tÃ¼")
            else:
                print(f"\nðŸ“Š {label}: âŒ Yetersiz veri (n={mask.sum()})")
        
        # Genel performans
        overall_r2 = r2_score(y_test, y_pred)
        overall_mape = mean_absolute_percentage_error(y_test, y_pred) * 100
        
        print(f"\nðŸŽ¯ GENEL PERFORMANS:")
        print(f"   â€¢ Genel RÂ² Skoru: {overall_r2:.4f}")
        print(f"   â€¢ Genel MAPE: %{overall_mape:.2f}")
        print(f"   â€¢ Toplam Test Verisi: {len(y_test)}")
        
        performance_results['overall'] = {
            'r2_score': float(overall_r2),
            'mape': float(overall_mape),
            'total_samples': int(len(y_test))
        }
        
        return performance_results
        
    except Exception as e:
        print(f"âŒ Performans analizi hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    # Veri setini yÃ¼kle
    print("Veri seti yÃ¼kleniyor...")
    data = load_and_preprocess_data()
    
    if data is not None:
        print(f"Veri seti baÅŸarÄ±yla yÃ¼klendi. Toplam {data.shape[0]} kayÄ±t, {data.shape[1]} Ã¶zellik var.")
        # Modeli eÄŸit
        model, scaler, feature_names = train_model(data)
        
        if model is not None:
            print("Model baÅŸarÄ±yla eÄŸitildi ve kaydedildi.") 